{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science Workflow\n",
    "\n",
    "## 1. Define the Problem\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### This problem definition\n",
    "1. Use Bigmart Sales data to predict the sales of other similar products and stores to the training set.\n",
    "2. Solving this problem helps with financial projects for stores along with stocking information\n",
    "3. Solving the problem manually would be to specify all the attributes of each item type and then look for patterns to be able to make predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import our favorite data science packages\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "# Import the libraries needed to import data from UCI into pandas\n",
    "import re\n",
    "import requests\n",
    "import io\n",
    "\n",
    "\n",
    "# We also will need a list of plotting libraries \n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.options.display.max_columns = 100\n",
    "\n",
    "# Obtain the training set\n",
    "url_name = \"https://raw.githubusercontent.com/3Blades/notebook-templates/master/python/Datasets/lp_train.csv\"\n",
    "request=requests.get(url_name).content\n",
    "df_train=pd.read_csv(io.StringIO(request.decode('utf-8')))\n",
    "df_train.head()\n",
    "\n",
    "# Obtain the test set\n",
    "url_name = \"https://raw.githubusercontent.com/3Blades/notebook-templates/master/python/Datasets/lp_test.csv\"\n",
    "request=requests.get(url_name).content\n",
    "df_test=pd.read_csv(io.StringIO(request.decode('utf-8')))\n",
    "df_test.head()\n",
    "\n",
    "# putting the data all together to clean it\n",
    "data = pd.concat([df_train, df_test], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare Data\n",
    "1. Data Selection. Availability, what is missing, what can be removed.\n",
    "2. Data Preprocessing. Organize selected data by formatting, cleaning and sampling.\n",
    "3. Data Transformation. Feature engineering using scaling, attribute decomposition and attribute aggregation.\n",
    "4. Data visualizations such as with histograms.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ApplicantIncome  CoapplicantIncome  Credit_History Dependents  \\\n",
      "0             5849                0.0             1.0          0   \n",
      "1             4583             1508.0             1.0          1   \n",
      "2             3000                0.0             1.0          0   \n",
      "3             2583             2358.0             1.0          0   \n",
      "4             6000                0.0             1.0          0   \n",
      "\n",
      "      Education Gender  LoanAmount  Loan_Amount_Term   Loan_ID Loan_Status  \\\n",
      "0      Graduate   Male         NaN             360.0  LP001002           Y   \n",
      "1      Graduate   Male       128.0             360.0  LP001003           N   \n",
      "2      Graduate   Male        66.0             360.0  LP001005           Y   \n",
      "3  Not Graduate   Male       120.0             360.0  LP001006           Y   \n",
      "4      Graduate   Male       141.0             360.0  LP001008           Y   \n",
      "\n",
      "  Married Property_Area Self_Employed  \n",
      "0      No         Urban            No  \n",
      "1     Yes         Rural            No  \n",
      "2     Yes         Urban           Yes  \n",
      "3     Yes         Urban            No  \n",
      "4      No         Urban            No  \n"
     ]
    }
   ],
   "source": [
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Understanding what data issues we have\n",
    "\n",
    "# This functions allows me to see 1st level of columns that need to be handled \n",
    "def dataCleaningAnalysis(df):\n",
    "    # columns with out numbers in them\n",
    "    all_cols = df.columns\n",
    "    num_cols = df._get_numeric_data().columns\n",
    "    nonum_cols = list(set(all_cols) - set(num_cols))\n",
    "\n",
    "    # columns and rows with nans in them\n",
    "    nancols = df.columns[pd.isnull(df).any()].tolist()\n",
    "\n",
    "    # columns with 0 in them or other numbers that should not be there\n",
    "    zerocols = df.columns[pd.notnull(df[df == 0]).any()].tolist()\n",
    "    return(nonum_cols, nancols, zerocols)\n",
    "\n",
    "nonum, nan, zero = dataCleaningAnalysis(data)\n",
    "print(\"Columns to look at and possible fix\\nNo num: {}\\nNan: {}\\nZero: {}\".format(nonum,nan,zero))\n",
    "\n",
    "# shows how many unique items for each column\n",
    "data.apply(lambda x: len(x.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fis the Item_Weight Column\n",
    "#Determine the average weight per item:\n",
    "item_avg_weight = data.pivot_table(values='Item_Weight', index='Item_Identifier')\n",
    "\n",
    "#Get a boolean variable specifying missing Item_Weight values\n",
    "miss_bool = data['Item_Weight'].isnull() \n",
    "\n",
    "#Impute data and check #missing values before and after imputation to confirm\n",
    "data.loc[miss_bool,'Item_Weight'] = data.loc[miss_bool,'Item_Identifier'].apply(lambda x: item_avg_weight[x])  \n",
    "\n",
    "print(sum(data['Item_Weight'].isnull()))\n",
    "\n",
    "# Make the last 8 weight equal to average weight of all items\n",
    "mn = data['Item_Weight'].mean()\n",
    "data.loc[data['Item_Weight'].isnull(),'Item_Weight'] = mn\n",
    "print(sum(data['Item_Weight'].isnull()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######### Fix Visibility being 0 with average visibility of each item category\n",
    "#Determine average visibility of a product\n",
    "visibility_avg = data.pivot_table(values='Item_Visibility', index='Item_Identifier')\n",
    "## Find 0 visibility and replace with mean visibility of that product:\n",
    "miss_bool = (data['Item_Visibility'] == 0)\n",
    "data.loc[miss_bool,'Item_Visibility'] = data.loc[miss_bool,'Item_Identifier'].apply(lambda x: visibility_avg[x])\n",
    "print(sum((data['Item_Visibility'] == 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Change outlet established years to number of years in business\n",
    "data['Outlet_Years'] = 2013 - data['Outlet_Establishment_Year']\n",
    "data['Outlet_Years'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### Crete Broad Category\n",
    "#Get the first two characters of ID:\n",
    "data['Item_Type_Combined'] = data['Item_Identifier'].apply(lambda x: x[0:2])\n",
    "#Rename them to more intuitive categories:\n",
    "data['Item_Type_Combined'] = data['Item_Type_Combined'].map({'FD':'Food',\n",
    "                                                             'NC':'Non-Consumable',\n",
    "                                                             'DR':'Drinks'})\n",
    "data['Item_Type_Combined'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Clean up Item_Fat_Content column\n",
    "data['Item_Fat_Content'] = data['Item_Fat_Content'].replace({'LF':'Low Fat', 'reg':'Regular', 'low fat':'Low Fat'})\n",
    "data['Item_Fat_Content'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "####### Add another value to non consumable items\n",
    "#Mark non-consumables as separate category in low_fat:\n",
    "data.loc[data['Item_Type_Combined']==\"Non-Consumable\",'Item_Fat_Content'] = \"Non-Edible\"\n",
    "data['Item_Fat_Content'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######### Fix Missing Data - Outlet Size using Store Type\n",
    "# For missing outlet size data figure out the mode of the outlet types and imput size based on that mode\n",
    "def modes(df, key_cols, value_col, count_col):\n",
    "    return df.groupby(key_cols + [value_col]).size() \\\n",
    "             .to_frame(count_col).reset_index() \\\n",
    "             .groupby(key_cols + [count_col])[value_col].unique() \\\n",
    "             .to_frame().reset_index() \\\n",
    "             .sort_values(count_col, ascending=False) \\\n",
    "             .drop_duplicates(subset=key_cols)\n",
    "\n",
    "print(modes(data,['Outlet_Type'],'Outlet_Size','count'),\"\\n\\n\")\n",
    "######### Fix Missing Data - Outlet Size using Store Type\n",
    "type_mode = {\"Supermarket Type1\":\"Small\",\"Supermarket Type\":\"Medium\",\"Supermarket Type2\":\"Medium\", \"Grocery Store\":\"Small\"}\n",
    "miss_bool = data['Outlet_Size'].isnull() \n",
    "data.loc[miss_bool,'Outlet_Size'] = data.loc[miss_bool,'Outlet_Type'].apply(lambda x: type_mode[x])\n",
    "print(sum(data['Outlet_Size'].isnull() ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Encode text data into numbers and take the spaces out of the column names\n",
    "# Fix Outlet_Size\n",
    "mapto = {\"Small\":1,\"Medium\":2, \"High\":3}\n",
    "data['Outlet_Size'] = data['Outlet_Size'].map(mapto)\n",
    "\n",
    "#One Hot Coding:\n",
    "data['Outlet'] = data['Outlet_Identifier']\n",
    "data = pd.get_dummies(data, columns=['Item_Fat_Content','Outlet_Location_Type','Outlet_Type','Item_Type_Combined','Outlet'])\n",
    "\n",
    "data.rename(columns=lambda x: x.replace(' ', '_'), inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Divide into test and train:\n",
    "train = data.loc[data['source']==\"train\"]\n",
    "test = data.loc[data['source']==\"test\"]\n",
    "\n",
    "#Drop unnecessary columns:\n",
    "test = test.drop(['source','Item_Type'],axis=1)\n",
    "train = train.drop(['source','Item_Type'],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is the final definition to solving the problem\n",
    "target = 'Item_Outlet_Sales'\n",
    "IDcol = ['Item_Identifier','Outlet_Identifier']\n",
    "predictors = [x for x in train.columns if x not in [target]+IDcol]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Your Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Spot Check Algorithms\n",
    "1. Test harness with default values.\n",
    "2. Run family of algorithms across all the transformed and scaled versions of dataset.\n",
    "3. View comparisons with box plots.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this uses the mean as a way of testing setting a base level for your algorithmns\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "predict =[]\n",
    "mean_sales = train['Item_Outlet_Sales'].mean()\n",
    "predict =  test[target].copy()\n",
    "predict[:] = mean_sales\n",
    "rmsbase = sqrt(mean_squared_error(test[target],predict))\n",
    "print(\"Base\", rmsbase)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spot Check Your Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Improve Results (Tuning)\n",
    "1. Algorithm Tuning: discovering the best models in model parameter space. This may include hyper parameter optimizations with additional helper services.\n",
    "2. Ensemble Methods: where the predictions made by multiple models are combined.\n",
    "3. Feature Engineering: where the attribute decomposition and aggregation seen in data preparation is tested further.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Using several techniques and testing the effectiveness on the data provided. \n",
    "# DecisionTreeRegressor depth 15 seems to be the of what we have tried\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import Ridge, Lasso, MultiTaskLasso, ElasticNet, BayesianRidge\n",
    "\n",
    "seed = 7\n",
    "scoring = 'accuracy'\n",
    "models = []\n",
    "models.append(('Lasso', Lasso()))\n",
    "models.append(('Ridge', Ridge(alpha=0.05,normalize=True)))\n",
    "models.append(('DecisionTreeRegressor', DecisionTreeRegressor(max_depth=4)))\n",
    "models.append(('ElasticNet', ElasticNet()))\n",
    "models.append(('BayesianRidge', BayesianRidge()))\n",
    "\n",
    "minrms = rmsbase\n",
    "for name, model in models:\n",
    "    model.fit(train[predictors],train[target])\n",
    "    predict = model.predict(test[predictors])\n",
    "    rms = sqrt(mean_squared_error(test[target],predict))\n",
    "    print(name, rms)\n",
    "    if(minrms > rms):\n",
    "        str = \"Best Algorithm is: {}\".format(name)\n",
    "        minrms = rms\n",
    "        \n",
    "print(\"\\n\\n{}\".format(str))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Improve Results (If not happy with above situation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Present Results\n",
    "1. Context (Why): how the problem definition arose in the first place.\n",
    "2. Problem (Question): describe the problem as a question.\n",
    "3. Solution (Answer): describe the answer the the question in the previous step.\n",
    "4. Findings: Bulleted lists of discoveries you made along the way that interests the audience. May include discoveries in the data, methods that did or did not work or the model performance benefits you observed.\n",
    "5. Limitations: describe where the model does not work.\n",
    "6. Conclusions (Why+Question+Answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "1. Problem is wanting to make sure we can supply consumers with what they want when they want it\n",
    "2. How can I make sure I have enough items but not too much of what a customer needs\n",
    "3. By knowing what my sales are going to be I can plan accordingly\n",
    "4. There are a broad set of items in the stores but they can be grouped into 3 categories. Stores of different types have different sizes. I know the list of items that are the big sellers across the board and those that are for a particular store. I can start to build some metrics that make up typical stores of a certain size or type.\n",
    "5. Limited to the data being used. As more data is acquired more knowledge will be gained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Present Your Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
