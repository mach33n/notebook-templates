{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eezzy Regression\n",
    "### Analyze a dataset of house prices with Eezzy functionality\n",
    "Eezzy ties in with powerful libraries like `pandas` and `scikit-learn` in order to allow data scientists to rapidly prototype their machine learning workflow. Within this notebook we'll perform exploratory data analysis on sold houses from 2014 to 2015, located in King County, USA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import re\n",
    "import requests\n",
    "import io\n",
    "sns.set_style('white')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preview\n",
    "Before getting our hands dirty with actual machine learning, we should first develop a understanding of the data at hand. A couple of things must be addressed at first:\n",
    "- Is there missing data?\n",
    "- Do some features have high collinearity?\n",
    "- What format is the data in?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>price</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>sqft_living</th>\n",
       "      <th>sqft_lot</th>\n",
       "      <th>floors</th>\n",
       "      <th>waterfront</th>\n",
       "      <th>view</th>\n",
       "      <th>...</th>\n",
       "      <th>grade</th>\n",
       "      <th>sqft_above</th>\n",
       "      <th>sqft_basement</th>\n",
       "      <th>yr_built</th>\n",
       "      <th>yr_renovated</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>sqft_living15</th>\n",
       "      <th>sqft_lot15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7129300520</td>\n",
       "      <td>20141013T000000</td>\n",
       "      <td>221900.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1180</td>\n",
       "      <td>5650</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1180</td>\n",
       "      <td>0</td>\n",
       "      <td>1955</td>\n",
       "      <td>0</td>\n",
       "      <td>98178</td>\n",
       "      <td>47.5112</td>\n",
       "      <td>-122.257</td>\n",
       "      <td>1340</td>\n",
       "      <td>5650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6414100192</td>\n",
       "      <td>20141209T000000</td>\n",
       "      <td>538000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.25</td>\n",
       "      <td>2570</td>\n",
       "      <td>7242</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>2170</td>\n",
       "      <td>400</td>\n",
       "      <td>1951</td>\n",
       "      <td>1991</td>\n",
       "      <td>98125</td>\n",
       "      <td>47.7210</td>\n",
       "      <td>-122.319</td>\n",
       "      <td>1690</td>\n",
       "      <td>7639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5631500400</td>\n",
       "      <td>20150225T000000</td>\n",
       "      <td>180000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.00</td>\n",
       "      <td>770</td>\n",
       "      <td>10000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>770</td>\n",
       "      <td>0</td>\n",
       "      <td>1933</td>\n",
       "      <td>0</td>\n",
       "      <td>98028</td>\n",
       "      <td>47.7379</td>\n",
       "      <td>-122.233</td>\n",
       "      <td>2720</td>\n",
       "      <td>8062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2487200875</td>\n",
       "      <td>20141209T000000</td>\n",
       "      <td>604000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3.00</td>\n",
       "      <td>1960</td>\n",
       "      <td>5000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1050</td>\n",
       "      <td>910</td>\n",
       "      <td>1965</td>\n",
       "      <td>0</td>\n",
       "      <td>98136</td>\n",
       "      <td>47.5208</td>\n",
       "      <td>-122.393</td>\n",
       "      <td>1360</td>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1954400510</td>\n",
       "      <td>20150218T000000</td>\n",
       "      <td>510000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1680</td>\n",
       "      <td>8080</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>1680</td>\n",
       "      <td>0</td>\n",
       "      <td>1987</td>\n",
       "      <td>0</td>\n",
       "      <td>98074</td>\n",
       "      <td>47.6168</td>\n",
       "      <td>-122.045</td>\n",
       "      <td>1800</td>\n",
       "      <td>7503</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id             date     price  bedrooms  bathrooms  sqft_living  \\\n",
       "0  7129300520  20141013T000000  221900.0         3       1.00         1180   \n",
       "1  6414100192  20141209T000000  538000.0         3       2.25         2570   \n",
       "2  5631500400  20150225T000000  180000.0         2       1.00          770   \n",
       "3  2487200875  20141209T000000  604000.0         4       3.00         1960   \n",
       "4  1954400510  20150218T000000  510000.0         3       2.00         1680   \n",
       "\n",
       "   sqft_lot  floors  waterfront  view     ...      grade  sqft_above  \\\n",
       "0      5650     1.0           0     0     ...          7        1180   \n",
       "1      7242     2.0           0     0     ...          7        2170   \n",
       "2     10000     1.0           0     0     ...          6         770   \n",
       "3      5000     1.0           0     0     ...          7        1050   \n",
       "4      8080     1.0           0     0     ...          8        1680   \n",
       "\n",
       "   sqft_basement  yr_built  yr_renovated  zipcode      lat     long  \\\n",
       "0              0      1955             0    98178  47.5112 -122.257   \n",
       "1            400      1951          1991    98125  47.7210 -122.319   \n",
       "2              0      1933             0    98028  47.7379 -122.233   \n",
       "3            910      1965             0    98136  47.5208 -122.393   \n",
       "4              0      1987             0    98074  47.6168 -122.045   \n",
       "\n",
       "   sqft_living15  sqft_lot15  \n",
       "0           1340        5650  \n",
       "1           1690        7639  \n",
       "2           2720        8062  \n",
       "3           1360        5000  \n",
       "4           1800        7503  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url_name = \"https://raw.githubusercontent.com/3Blades/notebook-templates/master/python/Datasets/Housing_Prices.csv\"\n",
    "request=requests.get(url_name).content\n",
    "df= pd.read_csv(io.StringIO(request.decode('utf-8')))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Looks like most of the data is immediately usable (numbers), although the date is in a string format. There are no NaN values_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_The average house in King County is $540,000, with around 3 bedrooms, 2 bathrooms, and 2080 sq ft._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def assess_collinearity(df):\n",
    "    combinations = []\n",
    "    collinearity = df.corr()\n",
    "    columns = collinearity.columns\n",
    "    for row in collinearity.itertuples(name=None):\n",
    "        index = row[0]\n",
    "        corr = row[1:]\n",
    "        for idx, value in enumerate(corr):\n",
    "            if value > .75 and index != columns[idx] and \\\n",
    "                    [columns[idx], index, value] not in combinations:\n",
    "                combinations.append([\n",
    "                    index,\n",
    "                    columns[idx],\n",
    "                    value\n",
    "                ])\n",
    "    print('Potentially Unsafe Features (High Collinearity):')\n",
    "    for risk in combinations:\n",
    "        print(\"The features {0} and {1} have a correlation rating of {2}\".format(*risk))\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(15, 15))\n",
    "    corr_arr = collinearity.values.ravel()\n",
    "    vmin = corr_arr.min()\n",
    "    vmax = corr_arr.max()\n",
    "    cax = ax.matshow(collinearity, vmin=vmin, vmax=vmax, cmap='BuGn')\n",
    "    fig.colorbar(cax)\n",
    "    ticks = np.arange(0, len(collinearity), 1)\n",
    "    plt.xticks(ticks, columns, rotation=90, size='large')\n",
    "    plt.yticks(ticks, columns, size='large')\n",
    "    ax.grid(False)\n",
    "    plt.show()\n",
    "assess_collinearity(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Impressions\n",
    "\n",
    "Looks like we'll need to watch out for high correlations between `sqft_living` and `sqft_above`. Additionally, we learned that our data is in an excellent format for the most part, although the feature `yr_renovated` is almost entirely empty, and could probably be best combined with another.\n",
    "\n",
    "---\n",
    "\n",
    "## Visualize the patterns\n",
    "Now that we have a sense of what the dataset is like, let's visualize how the house price is dependent on various features, and what trends we may see. Eezzy provides functionality for easy plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tbs.plotting.tbs_plotly_plot import plot_time_series\n",
    "plot_time_series(df[:1000], 'date', 'price', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "weekly_mean_df = df.resample('W', on='date').mean()\n",
    "weeks, weekly_means = weekly_mean_df.index, weekly_mean_df['price']\n",
    "plt.plot(weeks, weekly_means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['date'] > '2015/05/15']['price']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So even though the mean price appears to take off towards the end of the timespan of the dataset, it's only because of an outlier at the very end, and the fact that there are only two houses sold in the final week of the data. So the time of the year a house is sold doesn't appear to have a direct impact on the price the house was sold for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import tbs.plotting.tbs_plot as viz\n",
    "continuous_columns = ['bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'floors', 'view', 'condition']\n",
    "reload(viz)\n",
    "viz.continuous_plots(df, continuous_columns, num_columns=3, figsize=(15, 12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create a heatmap with longitude and latitude\n",
    "def heat(df, feature_names):\n",
    "    X, Y, V = tuple(df[name].values for name in feature_names)\n",
    "\n",
    "    xmin = X.min()\n",
    "    xmax = X.max()\n",
    "    ymin = Y.min()\n",
    "    ymax = Y.max()\n",
    "    vmin = V.min()\n",
    "    vmax = V.max()\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.subplots_adjust(hspace=0.5)\n",
    "    plt.hexbin(X, Y, C=V, vmin=vmin, vmax=vmax, cmap=plt.cm.YlOrRd_r)\n",
    "\n",
    "    plt.colorbar()\n",
    "    plt.axis([xmin, xmax, ymin, ymax])\n",
    "    plt.title('{2} over {0} and {1}'.format(*feature_names), size='xx-large')\n",
    "    plt.xlabel(feature_names[0], size='x-large')\n",
    "    plt.ylabel(feature_names[1], size='x-large')\n",
    "\n",
    "    plt.show()\n",
    "heat(df, ('long', 'lat', 'price'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Averages\n",
    "`plot_mean_over_x` will display the mean y value (in this case price) over x, which can also be a categorical feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def _averages(X, y, slices):\n",
    "    x_axis, dist, averages, stderrs = tuple(np.ones(slices.shape) for _ in range(4))\n",
    "    for i, left_index in enumerate(slices):\n",
    "        left_index = slices[i]\n",
    "        if i + 1 == len(slices):\n",
    "            right_index = -1\n",
    "        else:\n",
    "            right_index = slices[i + 1]\n",
    "        x_axis[i] = X[left_index:right_index].mean()\n",
    "        dist[i] = X[left_index: right_index].shape[0]\n",
    "        averages[i] = y[left_index:right_index].mean()\n",
    "        stderrs[i] = y[left_index:right_index].std()\n",
    "    return x_axis, dist, averages, stderrs\n",
    "\n",
    "def plot_running_mean(df, x_name, y_name, groups=20):\n",
    "    from matplotlib.patches import Patch\n",
    "    \n",
    "    g = sns.JointGrid(x=x_name, y=y_name, data=df)\n",
    "    X, y = df[x_name].values, df[y_name].values\n",
    "    \n",
    "    uniques = np.unique(X)\n",
    "    indices = np.argsort(X)\n",
    "    X = X[indices]\n",
    "    y = y[indices]\n",
    "    \n",
    "    step = len(X) // groups    \n",
    "    slices = np.arange(0, len(X), step)\n",
    "    x_axis, dist, averages, stderrs = _averages(X, y, slices) \n",
    "    \n",
    "    def joint(*args):\n",
    "        plt.plot(x_axis, averages)\n",
    "        \n",
    "    def marginal(*args, **kwargs):\n",
    "        plt.plot(x_axis, stderrs, color='#d16e6e')\n",
    "\n",
    "    joint_legend = Patch(color='C0', label=y_name)\n",
    "    ax_legend = Patch(color='#d16e6e', label='stderr')\n",
    "    \n",
    "    g.plot_joint(joint)\n",
    "    g.plot_marginals(marginal)\n",
    "    g.ax_marg_y.clear()\n",
    "    g.ax_marg_y.set_axis_off()\n",
    "\n",
    "    g.ax_joint.set_ylim(min(averages), max(averages))\n",
    "    plt.legend(handles=[joint_legend, ax_legend])\n",
    "    plt.suptitle('{0} over {1}'.format(x_name, y_name))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_category_mean(df, x_name, y_name, error=True):\n",
    "    X, y = df[x_name].values, df[y_name].values\n",
    "    uniques = set(X)\n",
    "    averages = [y[X == unique].mean() for unique in uniques]\n",
    "    stderrs = [y[X == unique].std() for unique in uniques]\n",
    "    x_axis = np.arange(len(uniques))\n",
    "    if error:\n",
    "        plt.bar(x_axis, averages, yerr=stderrs)\n",
    "    else:\n",
    "        plt.bar(x_axis, averages)\n",
    "    plt.xticks(x_axis, uniques, multialignment='center')\n",
    "    plt.xlabel(x_name)\n",
    "    plt.ylabel(y_name)\n",
    "    plt.suptitle('{0} over {1}'.format(x_name, y_name))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_running_mean(df, 'yr_built', 'price')\n",
    "plot_category_mean(df, 'condition', 'price')\n",
    "plot_category_mean(df, 'grade', 'price')\n",
    "plot_category_mean(df, 'view', 'price')\n",
    "plot_running_mean(df, 'sqft_living', 'price')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Looks like the 1940s was a rough time to have a house_\n",
    "\n",
    "_Grade has an excellent fit to average price, as does square feet of living_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slimming Down the Number of Features\n",
    "### The Curse of Dimensionality\n",
    "Our dataset has several features that are rather redundant or display no correlation with price. In order to improve the accuracy of the model, we'll remove or alter these features that could otherwise distract the model and cause it to underperform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First off, we don't need separate features for year built and year renovated\n",
    "# especially considering most houses haven't been renovated, meaning that\n",
    "# the renovated year feature is largely useless. So instead, we'll combine the two\n",
    "\n",
    "years_built = df['yr_built'].values\n",
    "years_ren = df['yr_renovated'].values\n",
    "def update_years(built, ren):\n",
    "    return built if built > ren else ren\n",
    "update_years = np.vectorize(update_years)\n",
    "\n",
    "new_feature = update_years(years_built, years_ren)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next we'll remove data points that aren't relevant for predicting the price\n",
    "train_df = df.drop([\n",
    "        'id',\n",
    "        'date',\n",
    "        'yr_renovated',\n",
    "        'view', # no good explanation for what this feature actually is\n",
    "        'condition', # grade offers a better insight than this does\n",
    "        'zipcode', # we already have longitude, latitude, and near-15 for geographic data\n",
    "        'sqft_above'\n",
    "    ], axis=1)\n",
    "# and finally, add in the newly composed feature\n",
    "train_df['yr_built'] = new_feature\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "names = train_df.columns\n",
    "scaler = StandardScaler()\n",
    "scaled_train_df = train_df.copy()\n",
    "for name in names:\n",
    "    if name != 'price':\n",
    "        scaled_train_df[name] = scaler.fit_transform(scaled_train_df[name].values.reshape(-1, 1))\n",
    "scaled_train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_That's a lot better - now the data is completely prepped and ready to be trained upon_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quickly Identifying Well-Performing Models\n",
    "Eezzy has the convinent method `eezzy_ml` that will take your dataset, assume what sort of problem it is (in this case regression), and score various models with various metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tbs.machine_learning import eezzy_ml\n",
    "exclusions = [\n",
    "    'AdaBoostRegressor',\n",
    "    'BaggingRegressor',\n",
    "    'ExtraTreesRegressor',\n",
    "    'GradientBoostingRegressor',\n",
    "    'RandomForestRegressor',\n",
    "    'GaussianProcessRegressor',\n",
    "    'KernelRidge',\n",
    "    'HuberRegressor',\n",
    "    'SGDRegressor',\n",
    "    'RadiusNeighborsRegressor',\n",
    "    'LinearSVR'\n",
    "]\n",
    "eezzy_ml(scaled_train_df, y='price', excluded=exclusions, reduce=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting a Model and Refining Results\n",
    "---\n",
    "Looks like decision trees and k-nearest neighbors fit the data the best. We'll import the model `DecisionTreeRegressor`, as it performed considerably better than others in the brief spotchecking and it provides an easy way of checking out feature importances. \n",
    "\n",
    "In order to get optimal results from the Decision Tree model, we'll try to create interaction variables in order to pull out any additional meaning from the data. We'll train it on all of the possible interaction variable features, and select the \"most important\" ones out of that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tbs.feature_engineering.tbs_transform import create_interactions\n",
    "interactions_df = create_interactions(train_df.drop(['price', 'bathrooms'], axis=1))\n",
    "\n",
    "for name in interactions_df.columns:\n",
    "    if np.any(np.isinf(interactions_df[name])):\n",
    "        interactions_df = interactions_df.drop(name, axis=1)\n",
    "\n",
    "for name in interactions_df.columns:\n",
    "    result = scaler.fit_transform(interactions_df[name].values.reshape(-1, 1))\n",
    "    interactions_df[name] = result\n",
    "interactions_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split, cross_val_predict\n",
    "from tbs.plotting.classification_plot import feature_importances\n",
    "\n",
    "best_params = {'criterion': 'mae', 'min_impurity_split': 1e-07}\n",
    "tree = DecisionTreeRegressor(**best_params)\n",
    "tree.fit(interactions_df.values, df['price'].values)\n",
    "feature_importances(interactions_df, tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll pull out the more important features and then compare the `scaled_train_df`, which has our normal data, and `interactions_df`, which has the interaction variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = tree.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "indices = indices[importances > 0.01]\n",
    "names = interactions_df.columns[indices]\n",
    "interactions_df[names].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "pred_y = cross_val_predict(tree, interactions_df.values, train_df['price'].values)\n",
    "r2_score(train_df['price'].values, pred_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y = cross_val_predict(tree, scaled_train_df.drop(['price'], axis=1).values, train_df['price'].values)\n",
    "r2_score(train_df['price'].values, pred_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "best_params = {'p': 1, 'leaf_size': 15, 'algorithm': 'ball_tree', 'n_neighbors': 5}\n",
    "neighbors = KNeighborsRegressor(**best_params)\n",
    "\n",
    "pred_y = cross_val_predict(neighbors, interactions_df.values, train_df['price'].values)\n",
    "r2_score(train_df['price'].values, pred_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y = cross_val_predict(neighbors, scaled_train_df.drop(['price'], axis=1).values, train_df['price'].values)\n",
    "r2_score(train_df['price'].values, pred_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
