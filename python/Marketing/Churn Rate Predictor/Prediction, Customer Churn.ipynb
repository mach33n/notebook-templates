{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 4,
        "hidden": false,
        "row": 0,
        "width": 4
       },
       "report_default": {
        "hidden": false
       }
      }
     }
    }
   },
   "source": [
    "# Customer Churn Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 6,
        "hidden": false,
        "row": 8,
        "width": 12
       },
       "report_default": {
        "hidden": false
       }
      }
     }
    }
   },
   "source": [
    "Customer Churn is the measure of the the amount of people who stop subscribing or buying a product over time. In situations where consumers are subscribing to a service, it is important to measure how likely those people are to stop subscribing because it may allow the marketing team to intervene in an attempt to stop him/her from churning. This can be accomplished by sending the user a discount code, reaching out to the user for a personalized review of the product in order to find pain points, etc. In this demo, when churn is true, it will hold the value of 1. When a customer does not churn, it will hold a value of 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 4,
        "height": 4,
        "hidden": false,
        "row": 0,
        "width": 4
       },
       "report_default": {
        "hidden": false
       }
      }
     }
    }
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 8,
        "height": 4,
        "hidden": false,
        "row": 0,
        "width": 4
       },
       "report_default": {
        "hidden": true
       }
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "# Import our favorite data science packages\n",
    "import scipy\n",
    "import importlib\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "# We also will need a list of plotting libraries \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# Import the libraries needed to import data from UCI into pandas\n",
    "import re\n",
    "import requests\n",
    "import io\n",
    "\n",
    "# Import sklearn libraries\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, auc, precision_score, roc_auc_score, roc_curve, confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Import the support libraries\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 4,
        "hidden": false,
        "row": 14,
        "width": 12
       },
       "report_default": {
        "hidden": false
       }
      }
     }
    }
   },
   "source": [
    "## Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 12,
        "hidden": false,
        "row": 30,
        "width": 4
       },
       "report_default": {
        "hidden": true
       }
      }
     }
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  State  Account Length  Area Code     Phone Int'l Plan VMail Plan  \\\n",
      "0    KS             128        415  382-4657         no        yes   \n",
      "1    OH             107        415  371-7191         no        yes   \n",
      "2    NJ             137        415  358-1921         no         no   \n",
      "3    OH              84        408  375-9999        yes         no   \n",
      "4    OK              75        415  330-6626        yes         no   \n",
      "\n",
      "   VMail Message  Day Mins  Day Calls  Day Charge   ...    Eve Calls  \\\n",
      "0             25     265.1        110       45.07   ...           99   \n",
      "1             26     161.6        123       27.47   ...          103   \n",
      "2              0     243.4        114       41.38   ...          110   \n",
      "3              0     299.4         71       50.90   ...           88   \n",
      "4              0     166.7        113       28.34   ...          122   \n",
      "\n",
      "   Eve Charge  Night Mins  Night Calls  Night Charge  Intl Mins  Intl Calls  \\\n",
      "0       16.78       244.7           91         11.01       10.0           3   \n",
      "1       16.62       254.4          103         11.45       13.7           3   \n",
      "2       10.30       162.6          104          7.32       12.2           5   \n",
      "3        5.26       196.9           89          8.86        6.6           7   \n",
      "4       12.61       186.9          121          8.41       10.1           3   \n",
      "\n",
      "   Intl Charge  CustServ Calls  Churn?  \n",
      "0         2.70               1  False.  \n",
      "1         3.70               1  False.  \n",
      "2         3.29               0  False.  \n",
      "3         1.78               2  False.  \n",
      "4         2.73               3  False.  \n",
      "\n",
      "[5 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "url_name = \"https://raw.githubusercontent.com/3Blades/notebook-templates/master/python/Datasets/Churn_Data.csv\"\n",
    "request=requests.get(url_name).content\n",
    "churn_data = pd.read_csv(io.StringIO(request.decode('utf-8')))\n",
    "print(churn_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "In this phase, we're going to clean the data to make it look a bit prettier. Some of the columns probably aren't going to be helpful in the long term and some of the columns need to be converted into numerical formats so that they can be inputted into sklearn and other machine learning model types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's now \n",
    "# First, we need to clean each column, including the column names\n",
    "churn_exp = churn_data.copy()\n",
    "churn_exp.rename(columns={'Churn?': 'Churn'}, inplace=True)\n",
    "\n",
    "# Now, you have to clean the columns\n",
    "churn_exp.loc[:, 'Churn'] = churn_exp.loc[:, 'Churn'].map(lambda x: re.sub('[.]',\n",
    "    '', x))\n",
    "\n",
    "# Now we need to convert the columns to something more usable. You also would need to know what columns need to be changed, \n",
    "# which can be a chore if you have hundreds of rows of data.\n",
    "\n",
    "churn_exp.loc[:, 'VMail Plan'].replace('yes', 1, inplace=True)\n",
    "churn_exp.loc[:, 'VMail Plan'].replace('no', 0, inplace=True)\n",
    "churn_exp.loc[:, 'Int\\'l Plan'].replace('yes', 1, inplace=True)\n",
    "churn_exp.loc[:, 'Int\\'l Plan'].replace('no', 0, inplace=True)\n",
    "\n",
    "# Now, we have to change the labels that we have to predict:\n",
    "\n",
    "churn_exp.loc[:, 'Churn'].replace('False', 0, inplace=True)\n",
    "churn_exp.loc[:, 'Churn'].replace('True', 1, inplace=True)\n",
    "\n",
    "# We now have to find and standardize the columns of the data for the algorithms like Logistic Regression.\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "standard_scaler = StandardScaler() # Assume .25, .75 quantiles\n",
    "# This will be a hard part.... \n",
    "# Also, it gives you a way to focus on what's important instead of being bombarded with a bunch of information at once. \n",
    "churn_exp[['Account Length', 'VMail Message', 'Day Mins', 'Day Calls', \n",
    "         'Day Charge', 'Eve Calls', 'Eve Charge', 'Night Mins', 'Eve Mins' ,\n",
    "         'Night Calls', 'Night Charge', 'Intl Mins', 'Intl Calls', \n",
    "         'Intl Charge', 'CustServ Calls']] =  StandardScaler().fit_transform(churn_exp[['Account Length', 'VMail Message',\n",
    "                                                                                        'Day Mins', 'Day Calls', \n",
    "         'Day Charge', 'Eve Calls', 'Eve Charge', 'Night Mins', 'Eve Mins',\n",
    "         'Night Calls', 'Night Charge', 'Intl Mins', 'Intl Calls', \n",
    "         'Intl Charge', 'CustServ Calls']])\n",
    "\n",
    "# Now, because each phone number is unique, we can simply drop it.\n",
    "churn_exp.drop('Phone', axis=1, inplace=True)\n",
    "print(churn_exp.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 5,
        "hidden": false,
        "row": 42,
        "width": 12
       },
       "report_default": {
        "hidden": false
       }
      }
     }
    }
   },
   "source": [
    "## Feature Engineering\n",
    "Normally, a data scientist would conduct exploratory data analysis in order to find out more information about the customer base. In this example, we're going to go straight into the prediction phase and attempt to make the customer base machine learning ready (the exploratory phase is covered in the Churn Plotly Notebook).  One of the most important parts of the data scientist's workflow is that we have to create the right features for us to input into the machine learning algorithm. The idea is that we have to ensure that the features are in the correct format while also maximizing information and minimizing the number of features to avoid curse of dimensionality problems with the data. Right now, we're attmepting to maxim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 12,
        "hidden": false,
        "row": 18,
        "width": 4
       },
       "report_default": {
        "hidden": true
       }
      }
     }
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# First we have to do something about the states. Do let's convert them to dummy variables.\n",
    "import sys\n",
    "print(churn_exp)\n",
    "state_columns = churn_exp['State'].unique()\n",
    "for state in state_columns:\n",
    "    churn_exp.loc[:, state] = (churn_exp.loc[:, 'State'] == state).astype('int')\n",
    "churn_exp.drop('State', axis=1, inplace=True)\n",
    "\n",
    "# You would also have to do something with area codes as well\n",
    "area_code_columns = churn_exp['Area Code'].unique()\n",
    "for area_code in area_code_columns:\n",
    "    churn_exp.loc[:, 'Area Code: ' + str(area_code)] = (churn_exp.loc[:, 'Area Code'] == area_code).astype('int')\n",
    "churn_exp.drop('Area Code', axis=1, inplace=True)\n",
    "\n",
    "# Now, we need to rename the column name.\n",
    "churn_exp.rename(columns={'Churn?' : \"Churn\"}, inplace=True)\n",
    "print(churn_exp.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 4,
        "height": 4,
        "hidden": false,
        "row": 18,
        "width": 4
       },
       "report_default": {
        "hidden": false
       }
      }
     }
    }
   },
   "source": [
    "## Model Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 26,
        "hidden": false,
        "row": 51,
        "width": 4
       },
       "report_default": {
        "hidden": true
       }
      }
     }
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "# Split the data between training and testing data.\n",
    "churn_exp = shuffle(churn_exp)\n",
    "cutoff_x = int(churn_exp.shape[0]*.80) # We assume an 80% split\n",
    "start_y = cutoff_x + 1\n",
    "X_train = churn_exp.drop('Churn', axis=1, inplace=False).iloc[0:cutoff_x, :]\n",
    "Y_train = churn_exp['Churn'][0:cutoff_x]\n",
    "X_test = churn_exp.drop('Churn', axis=1, inplace=False).iloc[start_y:, :]\n",
    "Y_test = churn_exp['Churn'][start_y:]\n",
    "\n",
    "# Score Matrices\n",
    "score_matrix = pd.DataFrame(columns=['Algorithm Name', 'Cross Validation Error', \n",
    "                                     'Accuracy', 'F1 Scores', 'Roc Auc Score', 'Precision', 'Recall'])\n",
    "\n",
    "# Decision Trees\n",
    "from sklearn import tree\n",
    "DT = tree.DecisionTreeClassifier()\n",
    "# The score difference between the highest and lowest model is X_score\n",
    "# Based on 3 different criteria, the average difference between the average \n",
    "DT.fit(X_train, Y_train)\n",
    "Y_pred = DT.predict(X_test)\n",
    "score_matrix.loc[0] = ['Decision Trees', np.mean(cross_val_score(DT, X_train, Y_train, scoring='roc_auc', cv=10)), \n",
    "                       accuracy_score(Y_test, Y_pred), f1_score(Y_test, Y_pred), roc_auc_score(Y_test, Y_pred), \n",
    "                       precision_score(Y_test, Y_pred),recall_score(Y_test, Y_pred)]\n",
    "\n",
    "objects = (\"Accuracy\", \"F1 Score\", \"Roc Auc Curve\", \"Precision\", \"Recall\")\n",
    "performance = [accuracy_score(Y_test, Y_pred), f1_score(Y_test, Y_pred), roc_auc_score(Y_test, Y_pred),\n",
    "    precision_score(Y_test, Y_pred), recall_score(Y_test, Y_pred)]\n",
    "\n",
    "\"\"\"\n",
    "# For those that prefer Matplotlib instead\n",
    "plt.bar(y_pos, performance, align='center', alpha=0.5)\n",
    "plt.xticks(y_pos, objects)\n",
    "plt.ylabel('Usage')\n",
    "plt.title(\"Decision Trees\")\n",
    "plt.show()\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Logsitic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logistic = LogisticRegression(penalty='l2', C=1e5)\n",
    "logistic.fit(X_train, Y_train)\n",
    "Y_pred = logistic.predict(X_test)\n",
    "score_matrix.loc[1] = ['Logistic Regression', np.mean(cross_val_score(logistic, X_train, Y_train, scoring='roc_auc', cv=10)), \n",
    "                       accuracy_score(Y_test, Y_pred), f1_score(Y_test, Y_pred), roc_auc_score(Y_test, Y_pred), \n",
    "                       precision_score(Y_test, Y_pred),recall_score(Y_test, Y_pred)]\n",
    "\n",
    "objects = (\"Accuracy\", \"F1 Score\", \"Roc Auc Curve\", \"Precision\", \"Recall\")\n",
    "y_pos = np.arange(len(objects))\n",
    "performance = [accuracy_score(Y_test, Y_pred), f1_score(Y_test, Y_pred), roc_auc_score(Y_test, Y_pred),\n",
    "    precision_score(Y_test, Y_pred), recall_score(Y_test, Y_pred)]\n",
    "\n",
    "\"\"\"\n",
    "# For those that prefer Matplotlib instead\n",
    "plt.bar(y_pos, performance, align='center', alpha=0.5)\n",
    "plt.xticks(y_pos, objects)\n",
    "plt.ylabel('Usage')\n",
    "plt.title(\"Logistic Regression\")\n",
    "plt.show()\n",
    "\"\"\"\n",
    "\n",
    "# Random Forests\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(n_estimators=10)\n",
    "rf.fit(X_train, Y_train)\n",
    "Y_pred = rf.predict(X_test)\n",
    "score_matrix.loc[2] = ['Random Forests', np.mean(cross_val_score(rf, X_train, Y_train, scoring='roc_auc', cv=10)), \n",
    "                       accuracy_score(Y_test, Y_pred), f1_score(Y_test, Y_pred), roc_auc_score(Y_test, Y_pred), \n",
    "                       precision_score(Y_test, Y_pred),recall_score(Y_test, Y_pred)]\n",
    "\n",
    "objects = (\"Accuracy\", \"F1 Score\", \"Roc Auc Curve\", \"Precision\", \"Recall\")\n",
    "y_pos = np.arange(len(objects))\n",
    "performance = [accuracy_score(Y_test, Y_pred), f1_score(Y_test, Y_pred), roc_auc_score(Y_test, Y_pred),\n",
    "    precision_score(Y_test, Y_pred), recall_score(Y_test, Y_pred)]\n",
    "\n",
    "\"\"\"\n",
    "# For those that prefer Matplotlib instead\n",
    "plt.bar(y_pos, performance, align='center', alpha=0.5)\n",
    "plt.xticks(y_pos, objects)\n",
    "plt.ylabel('Usage')\n",
    "plt.title('Random Forests')\n",
    "plt.show()\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# K Nearest Neighbors\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors=5, weights='uniform')\n",
    "knn.fit(X_train, Y_train)\n",
    "# Find the optimal parameters (Yuo would have to gridsearch or random search them)\n",
    "Y_pred = knn.predict(X_test)\n",
    "objects = (\"Accuracy\", \"F1 Score\", \"Roc Auc Curve\", \"Precision\", \"Recall\")\n",
    "y_pos = np.arange(len(objects))\n",
    "score_matrix.loc[3] = ['K Nearest Neighbors', np.mean(cross_val_score(knn, X_train, Y_train, scoring='roc_auc', cv=10)), \n",
    "                       accuracy_score(Y_test, Y_pred), f1_score(Y_test, Y_pred), roc_auc_score(Y_test, Y_pred), \n",
    "                       precision_score(Y_test, Y_pred),recall_score(Y_test, Y_pred)]\n",
    "\"\"\"\n",
    "# For those that prefer Matplotlib instead\n",
    "plt.bar(y_pos, performance, align='center', alpha=0.5)\n",
    "plt.xticks(y_pos, objects)\n",
    "plt.ylabel('Usage')\n",
    "plt.title(\"K Nearest Neighbors\")\n",
    "plt.show()\n",
    "\"\"\"\n",
    "\n",
    "display(score_matrix)\n",
    "\n",
    "fig, axs = plt.subplots(nrows=3, ncols=2, figsize=(12, 12))\n",
    "\n",
    "ax = sns.barplot(x='Algorithm Name', y=\"Cross Validation Error\", data=score_matrix, ax=axs[0][0])\n",
    "ax.set(ylabel='Score', title='Cross Validation Score')\n",
    "\n",
    "ax2 = sns.barplot(x='Algorithm Name', y=\"Accuracy\", data=score_matrix, ax=axs[0][1])\n",
    "ax2.set(ylabel='Score', title='Accuracy')\n",
    "\n",
    "ax3 = sns.barplot(x='Algorithm Name', y=\"F1 Scores\", data=score_matrix, ax=axs[1][0])\n",
    "ax3.set(ylabel='Score', title='F1 Scores')\n",
    "\n",
    "ax4 = sns.barplot(x='Algorithm Name', y=\"Roc Auc Score\", data=score_matrix, ax=axs[1][1])\n",
    "ax4.set(ylabel='Score', title='Roc Auc Score')\n",
    "\n",
    "ax5 = sns.barplot(x='Algorithm Name', y=\"Precision\", data=score_matrix, ax=axs[2][0])\n",
    "ax5.set(ylabel='Score', title='Precision')\n",
    "\n",
    "ax6 = sns.barplot(x='Algorithm Name', y=\"Recall\", data=score_matrix, ax=axs[2][1])\n",
    "ax6.set(ylabel='Score', title='Recall')\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "extensions": {
   "jupyter_dashboards": {
    "activeView": "report_default",
    "version": 1,
    "views": {
     "grid_default": {
      "cellMargin": 10,
      "defaultCellHeight": 20,
      "maxColumns": 12,
      "name": "grid",
      "type": "grid"
     },
     "report_default": {
      "name": "report",
      "type": "report"
     }
    }
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
