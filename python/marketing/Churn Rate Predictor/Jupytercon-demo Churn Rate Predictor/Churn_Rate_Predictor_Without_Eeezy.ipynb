{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 4,
        "hidden": false,
        "row": 0,
        "width": 4
       },
       "report_default": {
        "hidden": false
       }
      }
     }
    }
   },
   "source": [
    "# Data Science Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 4,
        "hidden": false,
        "row": 4,
        "width": 12
       },
       "report_default": {
        "hidden": false
       }
      }
     }
    }
   },
   "source": [
    "## Problem Definition: Using customer data, we want to predict which customers are going to churn or may leave the subscription that you provide."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 6,
        "hidden": false,
        "row": 8,
        "width": 12
       },
       "report_default": {
        "hidden": false
       }
      }
     }
    }
   },
   "source": [
    "### Further Information:\n",
    "### What is Churn?\n",
    "Churn is the measure of the the amount of people who stop subscribing or buying a product over time. In situations where consumers are subscribing to a service it is important to measure how likely those people are to stop subscribing. In this demo, when churn is true, it will hold the value of 1. When a customer does not churn, it will hold a value of 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 4,
        "height": 4,
        "hidden": false,
        "row": 0,
        "width": 4
       },
       "report_default": {
        "hidden": false
       }
      }
     }
    }
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 8,
        "height": 4,
        "hidden": false,
        "row": 0,
        "width": 4
       },
       "report_default": {
        "hidden": true
       }
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import scipy\n",
    "import importlib\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import matplotlib\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, auc, precision_score, roc_auc_score, roc_curve\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 4,
        "hidden": false,
        "row": 14,
        "width": 12
       },
       "report_default": {
        "hidden": false
       }
      }
     }
    }
   },
   "source": [
    "## Data Collection\n",
    "\n",
    "This phase of the project is where we connect to an external data stream such as a SQL database, an S3 bucket, or a simple CSV file so that we can gain access to our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 12,
        "hidden": false,
        "row": 30,
        "width": 4
       },
       "report_default": {
        "hidden": true
       }
      }
     }
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "churn_data = pd.read_hdf(\"C:\\\\Users\\\\elijah2352\\\\Downloads\\\\churndata.h5\")\n",
    "print(churn_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 5,
        "hidden": false,
        "row": 42,
        "width": 12
       },
       "report_default": {
        "hidden": false
       }
      }
     }
    }
   },
   "source": [
    "## Feature Engineering\n",
    "One of the most important parts of the data scientist's workflow is that we have to create the right features for us to input into the machine learning algorithm. The idea is that we have to ensure that the features are in the correct format while also maximizing information and minimizing the number of features to avoid curse of dimensionality problems with the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 12,
        "hidden": false,
        "row": 18,
        "width": 4
       },
       "report_default": {
        "hidden": true
       }
      }
     }
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# First, we need to clean each column, including the column names\n",
    "churn_exp = churn_data.copy()\n",
    "churn_exp.rename(columns={'Churn?': 'Churn'}, inplace=True)\n",
    "\n",
    "# Now, you have to clean the columns\n",
    "churn_exp.loc[:, 'Churn'] = churn_exp.loc[:, 'Churn'].map(lambda x: re.sub('[.]',\n",
    "    '', x))\n",
    "\n",
    "# Now we need to convert the columns to something more usable. You also would need to know what columns need to be changed, \n",
    "# which can be a chore if you have hundreds of rows of data.\n",
    "\n",
    "churn_exp.loc[:, 'VMail Plan'].replace('yes', 1, inplace=True)\n",
    "churn_exp.loc[:, 'VMail Plan'].replace('no', 0, inplace=True)\n",
    "churn_exp.loc[:, 'Int\\'l Plan'].replace('yes', 1, inplace=True)\n",
    "churn_exp.loc[:, 'Int\\'l Plan'].replace('no', 0, inplace=True)\n",
    "\n",
    "# Now, we have to change the labels that we have to predict:\n",
    "\n",
    "churn_exp.loc[:, 'Churn'].replace('False', 0, inplace=True)\n",
    "churn_exp.loc[:, 'Churn'].replace('True', 1, inplace=True)\n",
    "\n",
    "# We now have to find and standardize the columns of the data for the algorithms like Logistic Regression.\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "standard_scaler = StandardScaler() # Assume .25, .75 quantiles\n",
    "# This will be a hard part.... \n",
    "# Also, it gives you a way to focus on what's important instead of being bombarded with a bunch of information at once. \n",
    "# It sort of gives you a good process to draw from.\n",
    "churn_exp[['Account Length', 'VMail Message', 'Day Mins', 'Day Calls', \n",
    "         'Day Charge', 'Eve Calls', 'Eve Charge', 'Night Mins', 'Eve Mins' ,\n",
    "         'Night Calls', 'Night Charge', 'Intl Mins', 'Intl Calls', \n",
    "         'Intl Charge', 'CustServ Calls']] =  StandardScaler().fit_transform(churn_exp[['Account Length', 'VMail Message',\n",
    "                                                                                        'Day Mins', 'Day Calls', \n",
    "         'Day Charge', 'Eve Calls', 'Eve Charge', 'Night Mins', 'Eve Mins',\n",
    "         'Night Calls', 'Night Charge', 'Intl Mins', 'Intl Calls', \n",
    "         'Intl Charge', 'CustServ Calls']])\n",
    "\n",
    "# Now we have to do something about the states. We convert them to dummy variables.\n",
    "state_columns = churn_exp['State'].unique()\n",
    "for state in state_columns:\n",
    "    churn_exp.loc[:, state] = (churn_exp.loc[:, 'State'] == state).astype('int')\n",
    "churn_exp.drop('State', axis=1, inplace=True)\n",
    "\n",
    "# You would also have to do something with area codes as well\n",
    "area_code_columns = churn_exp['Area Code'].unique()\n",
    "for area_code in area_code_columns:\n",
    "    churn_exp.loc[:, 'Area Code: ' + str(area_code)] = (churn_exp.loc[:, 'Area Code'] == area_code).astype('int')\n",
    "churn_exp.drop('Area Code', axis=1, inplace=True)\n",
    "\n",
    "# Now, because each phone number is unique, we can simply drop it.\n",
    "churn_exp.drop(['Phone'], axis=1, inplace=True)\n",
    "\n",
    "# Now, we need to rename the column head.\n",
    "churn_exp.rename(columns={'Churn?' : \"Churn\"}, inplace=True)\n",
    "print(churn_exp.head())\n",
    "\n",
    "# Not only do you have to know the code above, but you also have to actually ensure that it works properly and the like. \n",
    "# With Eezzy, you can just have the code right there at your disposal. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 4,
        "height": 4,
        "hidden": false,
        "row": 18,
        "width": 4
       },
       "report_default": {
        "hidden": false
       }
      }
     }
    }
   },
   "source": [
    "## Model Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 26,
        "hidden": false,
        "row": 51,
        "width": 4
       },
       "report_default": {
        "hidden": true
       }
      }
     }
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Split the data between training and testing data.\n",
    "churn_exp = shuffle(churn_exp)\n",
    "cutoff_x = int(churn_exp.shape[0]*.80)\n",
    "start_y = cutoff_x + 1\n",
    "X_train = churn_exp.drop('Churn', axis=1, inplace=False).iloc[0:cutoff_x, :]\n",
    "Y_train = churn_exp['Churn'][0:cutoff_x]\n",
    "X_test = churn_exp.drop('Churn', axis=1, inplace=False).iloc[start_y:, :]\n",
    "Y_test = churn_exp['Churn'][start_y:]\n",
    "# Here, you have to know the  \n",
    "\n",
    "# Decision Trees\n",
    "from sklearn import tree\n",
    "DT = tree.DecisionTreeClassifier()\n",
    "print(\"Decision Trees\")\n",
    "# The score difference between the highest and lowest model is X_score\n",
    "# Based on 3 different criteria, the average difference between the average \n",
    "DT.fit(X_train, Y_train)\n",
    "print(\"Decision Trees Average CV Error: \" + str(np.mean(cross_val_score(DT, X_train, Y_train, scoring='roc_auc', cv=10))))\n",
    "Y_pred = DT.predict(X_test)\n",
    "print(\"Accuracy: \" + str(accuracy_score(Y_test, Y_pred)))\n",
    "print(\"F1 Score: \" + str(f1_score(Y_test, Y_pred)))\n",
    "print(\"Roc Ruc Score: \" + str(roc_auc_score(Y_test, Y_pred)))\n",
    "print(\"Precision: \" + str(precision_score(Y_test, Y_pred)))\n",
    "print(\"Recall: \" + str(recall_score(Y_test, Y_pred)))\n",
    "\n",
    "objects = (\"Accuracy\", \"F1 Score\", \"Roc Auc Curve\", \"Precision\", \"Recall\")\n",
    "y_pos = np.arange(len(objects))\n",
    "performance = [accuracy_score(Y_test, Y_pred), f1_score(Y_test, Y_pred), roc_auc_score(Y_test, Y_pred),\n",
    "    precision_score(Y_test, Y_pred), recall_score(Y_test, Y_pred)]\n",
    " \n",
    "plt.bar(y_pos, performance, align='center', alpha=0.5)\n",
    "plt.xticks(y_pos, objects)\n",
    "plt.ylabel('Usage')\n",
    "plt.title('Programming language usage')\n",
    " \n",
    "plt.show()\n",
    "\n",
    "# Logsitic Regression\n",
    "print(\"Logistic Regression\")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logistic = LogisticRegression(penalty='l2', C=1e5)\n",
    "logistic.fit(X_train, Y_train)\n",
    "print(\"Logistic Regression Average CV Error: \" + str(np.mean(cross_val_score(logistic, \n",
    "                                                                             X_train, Y_train, scoring='roc_auc', cv=10))))\n",
    "Y_pred = logistic.predict(X_test)\n",
    "print(\"Accuracy: \" + str(accuracy_score(Y_test, Y_pred)))\n",
    "print(\"F1 Score: \" + str(f1_score(Y_test, Y_pred)))\n",
    "print(\"Roc Ruc Score: \" + str(roc_auc_score(Y_test, Y_pred)))\n",
    "print(\"Precision: \" + str(precision_score(Y_test, Y_pred)))\n",
    "print(\"Recall: \" + str(recall_score(Y_test, Y_pred)))\n",
    "\n",
    "objects = (\"Accuracy\", \"F1 Score\", \"Roc Auc Curve\", \"Precision\", \"Recall\")\n",
    "y_pos = np.arange(len(objects))\n",
    "performance = [accuracy_score(Y_test, Y_pred), f1_score(Y_test, Y_pred), roc_auc_score(Y_test, Y_pred),\n",
    "    precision_score(Y_test, Y_pred), recall_score(Y_test, Y_pred)]\n",
    " \n",
    "plt.bar(y_pos, performance, align='center', alpha=0.5)\n",
    "plt.xticks(y_pos, objects)\n",
    "plt.ylabel('Usage')\n",
    "plt.title('Programming language usage')\n",
    " \n",
    "plt.show()\n",
    "\n",
    "# Random Forests\n",
    "print(\"Random Forests\")\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(n_estimators=10)\n",
    "rf.fit(X_train, Y_train)\n",
    "print(\"Random Forests Average CV Error: \" + str(np.mean(cross_val_score(rf, \n",
    "                                                                        X_train, Y_train, scoring='roc_auc', cv=10))))\n",
    "Y_pred = rf.predict(X_test)\n",
    "print(\"Accuracy: \" + str(accuracy_score(Y_test, Y_pred)))\n",
    "print(\"F1 Score: \" + str(f1_score(Y_test, Y_pred)))\n",
    "print(\"Roc Ruc Score: \" + str(roc_auc_score(Y_test, Y_pred)))\n",
    "print(\"Precision: \" + str(precision_score(Y_test, Y_pred)))\n",
    "print(\"Recall: \" + str(recall_score(Y_test, Y_pred)))\n",
    "\n",
    "objects = (\"Accuracy\", \"F1 Score\", \"Roc Auc Curve\", \"Precision\", \"Recall\")\n",
    "y_pos = np.arange(len(objects))\n",
    "performance = [accuracy_score(Y_test, Y_pred), f1_score(Y_test, Y_pred), roc_auc_score(Y_test, Y_pred),\n",
    "    precision_score(Y_test, Y_pred), recall_score(Y_test, Y_pred)]\n",
    " \n",
    "plt.bar(y_pos, performance, align='center', alpha=0.5)\n",
    "plt.xticks(y_pos, objects)\n",
    "plt.ylabel('Usage')\n",
    "plt.title('Programming language usage')\n",
    " \n",
    "plt.show()\n",
    "\n",
    "# K Nearest Neighbors\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "print(\"K Nearest Neighbors\")\n",
    "knn = KNeighborsClassifier(n_neighbors=5, weights='uniform')\n",
    "knn.fit(X_train, Y_train)\n",
    "# Find the optimal parameters (Yuo would have to gridsearch or random search them)\n",
    "print(\"K Nearest Neighbors Average CV Error:\" + str(np.mean(cross_val_score(knn, \n",
    "                                                                            X_train, Y_train, scoring='roc_auc', cv=10))))\n",
    "Y_pred = knn.predict(X_test)\n",
    "print(\"Accuracy: \" + str(accuracy_score(Y_test, Y_pred)))\n",
    "print(\"F1 Score: \" + str(f1_score(Y_test, Y_pred)))\n",
    "print(\"Roc Ruc Score: \" + str(roc_auc_score(Y_test, Y_pred)))\n",
    "print(\"Precision: \" + str(precision_score(Y_test, Y_pred)))\n",
    "print(\"Recall: \" + str(recall_score(Y_test, Y_pred)))\n",
    "\n",
    "objects = (\"Accuracy\", \"F1 Score\", \"Roc Auc Curve\", \"Precision\", \"Recall\")\n",
    "y_pos = np.arange(len(objects))\n",
    "performance = [accuracy_score(Y_test, Y_pred), f1_score(Y_test, Y_pred), roc_auc_score(Y_test, Y_pred),\n",
    "    precision_score(Y_test, Y_pred), recall_score(Y_test, Y_pred)]\n",
    " \n",
    "plt.bar(y_pos, performance, align='center', alpha=0.5)\n",
    "plt.xticks(y_pos, objects)\n",
    "plt.ylabel('Usage')\n",
    "plt.title('Programming language usage')\n",
    " \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "#TODO: Learning Curve\n",
    "train_sizes, train_scores, valid_scores = learning_curve(knn, X_train, Y_train, train_sizes = np.arange(.10, 1, .10), cv=10)\n",
    "\n",
    "\n",
    "\n",
    "#TODO: Probability Charts\n",
    "#TODO: Correlation Charts (Done, but needs to be done in machine learning graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tbs_plot.ml_plot_learning_curve(train_scores, valid_scores)"
   ]
  }
 ],
 "metadata": {
  "extensions": {
   "jupyter_dashboards": {
    "activeView": "report_default",
    "version": 1,
    "views": {
     "grid_default": {
      "cellMargin": 10,
      "defaultCellHeight": 20,
      "maxColumns": 12,
      "name": "grid",
      "type": "grid"
     },
     "report_default": {
      "name": "report",
      "type": "report"
     }
    }
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
